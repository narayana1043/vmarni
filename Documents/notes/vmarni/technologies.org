#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+STARTUP: showall
#+EXPORT_FILE_NAME: ../exports/technologies.html
#+OPTIONS: ^:nil

* Tinker Pop
** No vendor lock in
- Tinker Pop is an abstraction layer over different graph databases so it avoids vendor lock in to a specific database or processor (spark or hadoop)
** Developers
- we can try different implementations with the same code and settle on the right one
- scalable
- limited impact when swtiching providers
- advances in state of graph databases are behind Tinker Pops api
** languages
- gremlin console
Germlin is most recongized in this world and thats a good place to get started.
Read [[http://tinkerpop.apache.org/docs/current/tutorials/getting-started/][Getting started with Germlin]]
- groovy
- java
- javascript
- python

* ETL (SPARK)

** Spark processing configs
*** spark
- 1. [[https://aws.amazon.com/blogs/big-data/best-practices-for-running-apache-spark-applications-using-amazon-ec2-spot-instances-with-amazon-emr/][spark best practices]]
- 2. [[https://aws.amazon.com/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/][spark best practices for ET]]
- [[https://aws.amazon.com/blogs/big-data/submitting-user-applications-with-spark-submit/][submitting your spark application]]
- for requesting more than 20 ec2 instances see this [[https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html][topic]].
- [[https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html][spark gc]] 
*** executor settings (read from the link above submitting your spark application)
- number of executors per node = total number of cores on node - 1 (this one will used to run system processes).
- total number of executors (--num-executors or spark.executors.insatances) = number of executors per node * number of instances - 1
- the maximum amount of memory that can be set for an executor is dependent on the yarn.nodemanager.resource.memory-mb property in yarn-site.xml
- -executor-memory or spark.executor.memory defines the amount of memory each executor process can use.- spark.yarn.executor.memoryOverHead is off-heap memory automatically added to the executor.memory, it default value is 0.1*executorMemory

