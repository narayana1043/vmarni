#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+STARTUP: showall
#+EXPORT_FILE_NAME: ../exports/spark.html
#+OPTIONS: ^:nil

* Overview

(SPARK is more of an ART than a SCIENCE)
 
It is fast and general purpose cluster computing system. It provides high-level API's in
 - Java
 - Scala
 - Python
 - R
 - and an optimized engine that supports general execution graphs.

It also a rich set of higher-level tools including
 - Spark SQL for SQL
 - structure data processing
 - MLlib for machine learning
 - GraphX for graph processing
 - Spark Streaming

Things to know.
 - Security is turned off by default in spark. (vulnerable to attack by default). see more [[https://spark.apache.org/docs/latest/security.html][here]].
 - Spark uses hadoops client libraries for HDFS and YARN. Downloads are prepackaged for only popular hadoop versions.
 - Runs on windows and unix machines as long we have Java in path.

** Interactive analysis
- Spark shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively.
- Available in either scala (which runs on the Java VM and is thus a good way to use existing Java libraries) or python.

** Dataset
- Sparks primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other datasets.
- Due to python's dynamic nature, we don't need the Dataset to be strongly-typed in Python. As a result, all Datasets in Python are Dataset[Row], and we call it DataFrame to be consistent with pandas or R dataframes.

* History
- spark is application built to demo mesos which took over the big table world.
- Before spark 2.0 the main programming interface of spark was the resilient distributed dataset (RDD). After Spark 2.0, RDDs are replaced by Dataset, which is strongly-type like an RDD, but with richer optimizations under the hood. Recommended to use dataset. See [[https://spark.apache.org/docs/latest/sql-programming-guide.html][SQL programming giude]] to get more information about the dataset.
- RDD are still supported. RDD programming guide is [[https://spark.apache.org/docs/latest/rdd-programming-guide.html][here]].

* Introduction
essentially a scheduling, monitoring, distributing engine for big data.

** where spark fits into the big data world?
- At center of the spark universe is the spark core.
  - it uses both memory and disk when processing data.
- traditional apis on the top of the spark core
  - scala
  - java
  - python
  - R
  - dataframes api
    - very similar to dataframe in python and R.
- Around sparkcore there are higher-level apis
  - spark-sql
    - a module for working with data which is made of rows and columns.
    - it used to work with special rdd called a schema rdd, which is now replaced with dataframes.
    - lot of the hive queries works out of the box with spark-sql as it uses the hive-metastore
    - there are jdbc & odbc connections. tools like tabelau will pass their queries to spark-sql through one of these mechanisms
  - spark streaming
    - way faster than apache strom or yahoo's s4.
  - MLlib
    - things like classification, clustering, regression, dim reduction etc.
    - there is any underlying lib called Jbreeze, Jblast.
    - correlations, random data generation, collabarative filtering.
  - GraphX
  - BlinkDB
    - alpha project
      - for running interactive queries.
      - run a query to get answer in 5 sec but a margin of error
      - if we are unhappy we can let the query run for longer to reduce margin of error.
    - Tachyon
      - a memory based distributed storage system that allows data sharing across cluster frameworks like hadoop and spark
      - there is a java like api for tachyon.
  - Resource managers
    - they coordinate the running and execution of spark.
    - modes in spark
      - local mode
      - yarn (apache yarn project)
      - mesos (stands for mediator in greek)
      - spark stand-alone
    - file systems
      - spark can read from almost any file system and any database that supports hadoop.
      - file systems
        - HDFS
        - S3
        - Local
        - openstax
    - buffers
      - flume and kafka can be buffers for spark streaming.

** why spark?
spark gives a unified engine instead of learning several different engines like
- dremel
- pregel
- giraph
- tex
- mahout
- s4
- impala
- storm
- drill etc etc

You can start with spark core and build knowledge on out layer like streaming, dataframes, sql etc depending on the use case.

** speed

- 10 to 100 times faster then map reduce.
  - traditionally in map reduce we read data from hdfs and write intermediate results back to hdfs and then read back from hdfs and do this over and over again, which is the cause of the main slow down when comes computing.
  - so people used to use oozie which shoots map-reduce jobs in order.
  - spark on the other hand keeps the intermediate results in memory and write the final results to anywhere.
- motivations for leverging memory
  - when a cpu reads data from memory the read speed is 10GB/s
  - if go down to spinning diks 100MB/s
  - if we go to ssd then it is 600MB/s
  - over network 1GB/s
  - over to another rack 0.1GB/s

** whitepapers
- [[https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf][spark cluster computing with working sets]]
- [[https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf][Resilient distributed datasets: A fault tolerant abstraction for in memory cluster computing]]
- [[https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf][Discretized Streams: Fault tolerant streaming computation at scale]]
- [[https://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf][spark sql: relational data processing in spark]]
- [[https://amplab.cs.berkeley.edu/wp-content/uploads/2014/09/graphx.pdf][GraphX: Graph prpcessing in a distributed dataflow framework]]
- [[https://sameeragarwal.github.io/blinkdb_eurosys13.pdf][BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very Large data]]
* RDD Fundamentals

- Rdd has partitions and the more partitions we have the more parallelism we have.
- The n partition RDD can be spread across several machines.
- In general the RDD will have between 1000-10000 partitions.
  - each partiion will require a thread of computation.
  - basically if we have 5 partitions to compute against the rdd we need 5 tasks each of which will run inside a jvm on a worker node
  - so in a hundred node cluster if we just have only 5 partitions we cann't use the full power of the cluster.
- RDD can be create in two ways
  - parallelize a collection
    - not generally used outside protying and testing because the whole data has to fit in driver memory
  - read data from an rdd from an external source.
    - read from s3 or HDFS
    - read from cassandra and other sources in slightly different way.
- Repartition or Coalesce
  - sometimes after the filtering some partitions might be empty. In that case we might need to repartition or coalesce to make sure the next steps are better parallelized
- every transformation in spark is lazy
  - spark builds a DAG (directed acyclical graph). It basically metadata to say which RDD depends on which RDD.
  - only when action like collect or write is called on the final RDD all the transformations are executed.
  - careful when calling collect. (might cause memory out errors). If the rdd is very large write it to HDFS
- count operations
  - each executor counts the number of rows in their local rdds and the driver add all the results together.
- caching rdd or presisting into memory
  - you should always be very careful which rdd you are caching. Be very selective.
  - it also ok if the RDD doesn't fit in memeory. Ex: if only 30% of the data fits into memory it ok the rest of the RDD is saved on to the disk from where spark will get it. (Resilient in Resilient Distributed Dataset comes from here)
  - speed depends on how much of the RDD fits in memory.
  - it also lazily cached which means that only when a action is called it actually start the caching.
  - There is setting called SPARK_LOCAL_DIRS which you can use to tell spark where to fill of the RDD that doesn't fit into memory when cached and spark will pick it up from there later when required. It is also used for shuffles as well. Use SSD's if you can. If there are multiple disks spark will push data down parallely to all the disks
  - One more thing to remember about caching is the amount memory taken for the cache is generally 2 or 3 times the memory taken in the disk. This overhead is due to the deserialization of the data.
  - Caching RDD is also help to understand the shape of the RDD. Example cache the RDD and take a look at the partitions in the Spark driver UI, if you notice that a lot the data is in just one partition. That is a really bad shape for the RDD. You might want to shuffle or do something about. This is one way to understand if your RDD is lopsided. For the ones that don't fit in memory the size on the disk will also be shown in the UI.

** RDDs are [[https://github.com/apache/spark/tree/master/core/src/main/scala/org/apache/spark/rdd][typed]]

  - hadoop rdd
  - filtered rdd
  - mapped rdd
  - pair rdd
  - shuffled rdd
  - union rdd
  - python rdd
  - double rdd
  - jdbc rdd
  - json rdd
  - schmea rdd
  - vertex rdd
  - edge rdd
  - cassandra rdd
  - geo rdd
  - EsSpark
  - [[https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala][scala rdd]]

** Life cycle of spark program
- create some input rdds from exernal data or parallelize a collection in your driver program
- lazily transform them to define new RDDs using transformations like filter() or map()
- Ask Spark to cache() any intermediate RDDs that will need to be reused.
- Lunch actions such as count() and collect() to kick off a parallel computation, whcih is then optimized and executed by spark.
 
** Transformations
Most transformations are element wise, but this not true for all transformations. If you want to do something like a rdd that has like 1000 partitions and each of these partition has 50 items, what you don't want to do is open a connection to database for each item and instead do it a partiion level, where you open one connection and get an iterator and push the items and then close the connection.

The apache docs will give like 2-3 lines of explanation for these transformations. If that is not helpful look up the spark source code in scala on github where you will find comments which explains what the transformation does.

- map
- flatmap
- filter
- mapPartitions
- mapPartiionsWithIndex
- sample
- union
- intersection
- distinct
- groupByKey
- reduceByKey
- sortByKey
- join
- cogroup
- cartesion
- pipe
- coalesce
- repartition
- partitionBy
  
** RDD Interface
This cpatures all cureent spark operations
- set of partitions ('splits')
- list of dependencies on parent rdd's
- function to compute a partition given parents
  - if there is an rdd with 50 partiions and loose the partition number 3 that you have cached. The RDD will now how to recreate the lost RDD given the parents. This might only require recreating the 3 partition of the parent or 3 partition of all the parents above it.
- Optional preferred location
  - for example if you are reading from hdfs, each rdd will prefer to live on a machine on which it has a replica of the hdfs block local to that machine.
- Optional partitioning info for key/value RDDs (partitioner)
  - An rdd can have a partitioner set on it. You can also have a custom partitioner set on it. 
* Spark Resource Managers
** You can run spark in one of the 4 different  ways
- Local (local machine) 
- Standalone Scheduler (cassandra datastax)
- YARN (hadoop) 
- Mesos

** Partitioning(not RDD partitioning)
2 types, Spark allows you to dynamically adjust the resources your application occupies based on the workload.
- static partitioning: 
  - local
- dynamic partitioning
  - YARN
  - Mesos
  - standalone scheduler

** Hadoop is actually 3 projects under Apache.
- HDFS
  - NameNode
  - DataNode
- YARN
- MapReduce
  - JobTracker
  - TaskTracker
    - Map
    - Reduce  

** Differences between spark and map-reduce
- Spark's purpose is not replace hadoop but to replace the map-reduce part of the Hadoop. The away you get parallelism in traditional map-reduce is you have multiple jvms and process ids for your map and reduce tasks. However in spark you get parallelism by having different threads running inside the executor.
- Map and reduce slots in traditional map-reduce are hard coded. The problem is initially when you start a map-reduce job the application is probably stuck for hours in map phase for hours and during this time the reduce slots are not used. Therefore we cannot see 90% CPU usage. In spark the slots are generic slots that can run map or reduce or any other thing.
- In traditional map reduce, once a map finishes what has to happen is the task tracker has to let the job tracker know that there is an empty slot so that it can assign a new map task. Therefore once a task finishes it could take 15-20 seconds before a new task lands in the slot (heartbeat b/w the job-tracker and task-tracker is 3 sec). So facebook built corona to overcome this issue. This is not a problem in spark. The reuse of slots in spark is significantly faster.
- spark actually calls the slots as cores.

** Local Mode
- start a scala/python shell
  - this starts a jvm
  - in this jvm you cache your rdd.
- setting the number of cores
  - assuming the machine has 8 cores, it a very common beginner mistake to say 2 for os and 6 for spark. This is a mistake
  - In general you should start with a factor of 2 or 3, which is equal to 12 or 18 task slots.
  - you can pass in a parameter at the start to of the shell to say how many threads you want in spark or spark starts with a default of one thread.
  - you pass * spark starts the threads which is equal to the number of logical cores, which is also by bad.
  - good setting is to use a factor of 2.
  - of course this might screw up the other services running on the machine. Therefore coming up with the right number of cores is an art of the system administrator.
- internal threads in spark.
  - roughly 20 internal threads running at anypoint in time. Mostly sitting ideal.
  - These are used for shuffle etc. When we need a shuffle they kick in, do the shuffle and go ideal again.
  - so if you try to align the number of logical cores with the number of tasks that won't work anyway because there are always internal threads running in spark.
  - shuffles in local mode are super fast as there are no data movement over the network.

** Standalone Mode
- In this mode when you start machines certain JVMs are going to instantiate. 
  - this is not starting an application
  - Master JVM
  - On each machine a worker JVM will start up
  - all the worker JVM will register with master and the master will know that there are 4 workers

The master and worker JVM are the resources managers. These JVMs are relatively small JVMs. They are relavtily ok with 500MB or 1GB of RAM.

Now we start the application:
- ssh into one of the machines.
- a driver starts in the machine where you ssh and asks the master for executors.
- The spark master works like a schedular to decide where it is going to launch the executor JVMs. Then tells each worker to launch the executor JVMs.
- All the master is doing is deciding and scheduling where the executors are going to run.
- All the worker jvms are doing is start the executors and if the executor crashes the worker restarts it. Thats why it doesnot need that much memory.
- If the worker jvm crashes the worker restarts it.
- You can make the driver highly available as well with a supervised tag, then if a driver fails the master restarts but if the driver restarts all the workers have to restart as well.
- The RDDs are cached in executors and the tasks are run in the executors. When the tasks are run local to an executor JVM which is what spark aims to do then it is quite fast, because the data from the executor JVMs heap right into the thread.
- You can also persist replicas on the RDDs.
- You can make the master highly available by adding more master jvms even while the application is running.
- If you are running multiple applications in standalone. You can have one worker start multiple executors, however one worker cannot start multiple executors for the same application.
  - This is a problem when you have machines with huge RAM. Because if we start a JVM with huge RAM then the garbage collection pauses gets really awkward.
  - Therefore if you want to utlize all the memory. Then you need to 2 or more workers and each worker on the machine can start a single executor.
  - SPARK_WORKER_INSTANCES (default = 1) this is how many worker jvms you want in each machine.
- SPARK_WORK_CORES (default = all) is the number of cores to allow spark applications to use on the machine. This is not the number of threads that will run in a worker JVM. It is actually the number of cores that the worker can give out to the underlaying executors. If you set this is 12 and one executor is started with 6 than it can start another executor with 6 cores. When you start an application in this mode and don't tell anything about the cores then the application will greedily acquire all the cores on the worker. By default it will whatever will take the max is.
- SPARK_WORKER_MEMORY (default = TOTAL RAM - 1GB) is how much the worker can give to its underlying executors. This is not setting the JVM heap the worker will use.
- SPARK_DEAMON_MEMORY (default = 512 MB) is the memory to allocate to spark master JVMs and the spark worker deamons themselves. The way you set the spark master and worker to set 1GB heaps is this one.
- Standlone mode settings
  - spark.cores.max: maximum of amount of CPU cores to request for the application from across the cluster.
  - spark.executor.memory: Memory for each executor

** YARN Mode

Yet Another Resource Negotiator

*** Architecture of YARN

**** Introduction

- There is a master machine running a resource manager. It contains
  - Schedular, which decides where the masters will run and the containers are scheduled
  - ApplicationS master, when an application master crashes this ApplicationS master will restart the application master and the application will have t o restart completely.
- Each slave machine in the hadoop cluster will run a node manager
- The node managers are heart beating with the Resource manager and sharing information live resource information.
  - How many cores are occupied
  - How much RAM is occupied
  - How much of the bandwidth is occupied
  This give the resource manager the ability to see how much resources are occupied across the cluster

**** Application Flow

You start the client application and submit it to the resource manager and it is gonna look at it and assume the responsibilty to negotiate a specific container to run on a node. 
- The application submitted will have some constraints for the application master and the resource manager will then find a node that has free resources that your application master needs and then start the application master through a container on that node.
- The application master is not going to do any work. It like the schedular in the traditional map reduce. What the application master will do is immediately contact the resource manager and say thanks for starting me but i need containers to do my actual work.
- The resource manager will then find the nodes that have the resources and then handover keys to the master with which the master can communicate with the node managers to start the containers.
- The node managers will accept the keys and will start the containers and register themselves with the application master and the application master will report this directly back to the client.
- After this point the containers are not in directly contact with the resource manager, so if the resource manager crashes the application will still be running. One problem if the resource manager crashes the application master will not be able to ask for new resources from the resource manger during the run time of this application. But if everything is fine the application master can be increasing and decreasing the number of containers while the application is running.
- With YARN the application master can make really detailed requests
  - One container with GPU
  - One container with CPU
  - Both containers should be on the same rack
  - Both containers should be on the same machine
  - Both container should have X amount of memory
 
*** Spark in YARN

There are 2 ways

**** client mode

- In client mode the driver runs on the client itself.
- The client can be your laptop.
- There are rest of thing like application master, containers, node managers, resource managers are still exactly as discussed above.
- However if you call an collect action on an RDD, the items inside the RDD will be shift to the driver over the network and you will be able to see the results in the shell. Not good if you collect the whole dataset.
- In this case the only reason the application master exsits is because in YARN that is the only way you can get containers to run executors. In this case the executors running in the containers are in direct communication in the driver.
- This is similar to the standalone mode where the worker start the containers, but here the node manager starts the containers and then starts the executors within it. But actually it is the application master that negotiates with the resource manager that it wants two containers to run executors.
- RM tells Application Master where the containers can be scheduled. The resource manager is basically doing what the spark master is doing in the standalone mode.
- However now there is one important schedular inside the spark driver which decides where the actual tasks will be scheduled. The driver schedular is going to aim for placing the tasks whereever the data is or closer to the data. This is different from the RM schdeular.

**** cluster mode

** Issues

*** Data locality

If you have a 100 node cluster start 100 executors so that you won't run into data locality issues. If you start less then data has to be over the network which can be a pain.
