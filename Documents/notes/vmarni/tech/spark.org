#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+STARTUP: showall
#+EXPORT_FILE_NAME: ../exports/spark.html
#+OPTIONS: ^:nil

* Overview

(SPARK is more of an ART than a SCIENCE)
 
It is fast and general purpose cluster computing system. It provides high-level API's in
 - Java
 - Scala
 - Python
 - R
 - and an optimized engine that supports general execution graphs.

It also a rich set of higher-level tools including
 - Spark SQL for SQL
 - structure data processing
 - MLlib for machine learning
 - GraphX for graph processing
 - Spark Streaming

Things to know.
 - Security is turned off by default in spark. (vulnerable to attack by default). see more [[https://spark.apache.org/docs/latest/security.html][here]].
 - Spark uses hadoops client libraries for HDFS and YARN. Downloads are prepackaged for only popular hadoop versions.
 - Runs on windows and unix machines as long we have Java in path.

** Interactive analysis
- Spark shell provides a simple way to learn the API, as well as a powerful tool to analyze data interactively.
- Available in either scala (which runs on the Java VM and is thus a good way to use existing Java libraries) or python.

** Dataset
- Sparks primary abstraction is a distributed collection of items called a Dataset. Datasets can be created from Hadoop InputFormats (such as HDFS files) or by transforming other datasets.
- Due to python's dynamic nature, we don't need the Dataset to be strongly-typed in Python. As a result, all Datasets in Python are Dataset[Row], and we call it DataFrame to be consistent with pandas or R dataframes.

* History
- spark is application built to demo mesos which took over the big table world.
- Before spark 2.0 the main programming interface of spark was the resilient distributed dataset (RDD). After Spark 2.0, RDDs are replaced by Dataset, which is strongly-type like an RDD, but with richer optimizations under the hood. Recommended to use dataset. See [[https://spark.apache.org/docs/latest/sql-programming-guide.html][SQL programming giude]] to get more information about the dataset.
- RDD are still supported. RDD programming guide is [[https://spark.apache.org/docs/latest/rdd-programming-guide.html][here]].

* Introduction
essentially a scheduling, monitoring, distributing engine for big data.

** where spark fits into the big data world?
- At center of the spark universe is the spark core.
  - it uses both memory and disk when processing data.
- traditional apis on the top of the spark core
  - scala
  - java
  - python
  - R
  - dataframes api
    - very similar to dataframe in python and R.
- Around sparkcore there are higher-level apis
  - spark-sql
    - a module for working with data which is made of rows and columns.
    - it used to work with special rdd called a schema rdd, which is now replaced with dataframes.
    - lot of the hive queries works out of the box with spark-sql as it uses the hive-metastore
    - there are jdbc & odbc connections. tools like tabelau will pass their queries to spark-sql through one of these mechanisms
  - spark streaming
    - way faster than apache strom or yahoo's s4.
  - MLlib
    - things like classification, clustering, regression, dim reduction etc.
    - there is any underlying lib called Jbreeze, Jblast.
    - correlations, random data generation, collabarative filtering.
  - GraphX
  - BlinkDB
    - alpha project
      - for running interactive queries.
      - run a query to get answer in 5 sec but a margin of error
      - if we are unhappy we can let the query run for longer to reduce margin of error.
    - Tachyon
      - a memory based distributed storage system that allows data sharing across cluster frameworks like hadoop and spark
      - there is a java like api for tachyon.
  - Resource managers
    - they coordinate the running and execution of spark.
    - modes in spark
      - local mode
      - yarn (apache yarn project)
      - mesos (stands for mediator in greek)
      - spark stand-alone
    - file systems
      - spark can read from almost any file system and any database that supports hadoop.
      - file systems
        - HDFS
        - S3
        - Local
        - openstax
    - buffers
      - flume and kafka can be buffers for spark streaming.

** why spark?
spark gives a unified engine instead of learning several different engines like
- dremel
- pregel
- giraph
- tex
- mahout
- s4
- impala
- storm
- drill etc etc

You can start with spark core and build knowledge on out layer like streaming, dataframes, sql etc depending on the use case.

** speed

- 10 to 100 times faster then map reduce.
  - traditionally in map reduce we read data from hdfs and write intermediate results back to hdfs and then read back from hdfs and do this over and over again, which is the cause of the main slow down when comes computing.
  - so people used to use oozie which shoots map-reduce jobs in order.
  - spark on the other hand keeps the intermediate results in memory and write the final results to anywhere.
- motivations for leverging memory
  - when a cpu reads data from memory the read speed is 10GB/s
  - if go down to spinning diks 100MB/s
  - if we go to ssd then it is 600MB/s
  - over network 1GB/s
  - over to another rack 0.1GB/s

** whitepapers
- [[https://www.usenix.org/legacy/event/hotcloud10/tech/full_papers/Zaharia.pdf][spark cluster computing with working sets]]
- [[https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf][Resilient distributed datasets: A fault tolerant abstraction for in memory cluster computing]]
- [[https://people.csail.mit.edu/matei/papers/2013/sosp_spark_streaming.pdf][Discretized Streams: Fault tolerant streaming computation at scale]]
- [[https://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf][spark sql: relational data processing in spark]]
- [[https://amplab.cs.berkeley.edu/wp-content/uploads/2014/09/graphx.pdf][GraphX: Graph prpcessing in a distributed dataflow framework]]
- [[https://sameeragarwal.github.io/blinkdb_eurosys13.pdf][BlinkDB: Queries with Bounded Errors and Bounded Response Times on Very Large data]]
* RDD Fundamentals

- Rdd has partitions and the more partitions we have the more parallelism we have.
- The n partition RDD can be spread across several machines.
- In general the RDD will have between 1000-10000 partitions.
  - each partiion will require a thread of computation.
  - basically if we have 5 partitions to compute against the rdd we need 5 tasks each of which will run inside a jvm on a worker node
  - so in a hundred node cluster if we just have only 5 partitions we cann't use the full power of the cluster.
- RDD can be create in two ways
  - parallelize a collection
    - not generally used outside protying and testing because the whole data has to fit in driver memory
  - read data from an rdd from an external source.
    - read from s3 or HDFS
    - read from cassandra and other sources in slightly different way.
- Repartition or Coalesce
  - sometimes after the filtering some partitions might be empty. In that case we might need to repartition or coalesce to make sure the next steps are better parallelized
- every transformation in spark is lazy
  - spark builds a DAG (directed acyclical graph). It basically metadata to say which RDD depends on which RDD.
  - only when action like collect or write is called on the final RDD all the transformations are executed.
  - careful when calling collect. (might cause memory out errors). If the rdd is very large write it to HDFS
- count operations
  - each executor counts the number of rows in their local rdds and the driver add all the results together.
- caching rdd or presisting into memory
  - you should always be very careful which rdd you are caching. Be very selective.
  - it also ok if the RDD doesn't fit in memeory. Ex: if only 30% of the data fits into memory it ok the rest of the RDD is saved on to the disk from where spark will get it. (Resilient in Resilient Distributed Dataset comes from here)
  - speed depends on how much of the RDD fits in memory.
  - it also lazily cached which means that only when a action is called it actually start the caching.
  - There is setting called SPARK_LOCAL_DIRS which you can use to tell spark where to fill of the RDD that doesn't fit into memory when cached and spark will pick it up from there later when required. It is also used for shuffles as well. Use SSD's if you can. If there are multiple disks spark will push data down parallely to all the disks
  - One more thing to remember about caching is the amount memory taken for the cache is generally 2 or 3 times the memory taken in the disk. This overhead is due to the deserialization of the data.
  - Caching RDD is also help to understand the shape of the RDD. Example cache the RDD and take a look at the partitions in the Spark driver UI, if you notice that a lot the data is in just one partition. That is a really bad shape for the RDD. You might want to shuffle or do something about. This is one way to understand if your RDD is lopsided. For the ones that don't fit in memory the size on the disk will also be shown in the UI.

** RDDs are [[https://github.com/apache/spark/tree/master/core/src/main/scala/org/apache/spark/rdd][typed]]

  - hadoop rdd
  - filtered rdd
  - mapped rdd
  - pair rdd
  - shuffled rdd
  - union rdd
  - python rdd
  - double rdd
  - jdbc rdd
  - json rdd
  - schmea rdd
  - vertex rdd
  - edge rdd
  - cassandra rdd
  - geo rdd
  - EsSpark
  - [[https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala][scala rdd]]

** Life cycle of spark program
- create some input rdds from exernal data or parallelize a collection in your driver program
- lazily transform them to define new RDDs using transformations like filter() or map()
- Ask Spark to cache() any intermediate RDDs that will need to be reused.
- Lunch actions such as count() and collect() to kick off a parallel computation, whcih is then optimized and executed by spark.
 
** Transformations
Most transformations are element wise, but this not true for all transformations. If you want to do something like a rdd that has like 1000 partitions and each of these partition has 50 items, what you don't want to do is open a connection to database for each item and instead do it a partiion level, where you open one connection and get an iterator and push the items and then close the connection.

The apache docs will give like 2-3 lines of explanation for these transformations. If that is not helpful look up the spark source code in scala on github where you will find comments which explains what the transformation does.

- map
- flatmap
- filter
- mapPartitions
- mapPartiionsWithIndex
- sample
- union
- intersection
- distinct
- groupByKey
- reduceByKey
- sortByKey
- join
- cogroup
- cartesion
- pipe
- coalesce
- repartition
- partitionBy
  
** RDD Interface
This cpatures all cureent spark operations
- set of partitions ('splits')
- list of dependencies on parent rdd's
- function to compute a partition given parents
  - if there is an rdd with 50 partiions and loose the partition number 3 that you have cached. The RDD will now how to recreate the lost RDD given the parents. This might only require recreating the 3 partition of the parent or 3 partition of all the parents above it.
- Optional preferred location
  - for example if you are reading from hdfs, each rdd will prefer to live on a machine on which it has a replica of the hdfs block local to that machine.
- Optional partitioning info for key/value RDDs (partitioner)
  - An rdd can have a partitioner set on it. You can also have a custom partitioner set on it. 
* Spark Resource Managers
** You can run spark in one of the 4 different  ways
- Local (local machine) 
- Standalone Scheduler (cassandra datastax)
- YARN (hadoop) 
- Mesos

** Partitioning(not RDD partitioning)
2 types, Spark allows you to dynamically adjust the resources your application occupies based on the workload.
- static partitioning: 
  - local
- dynamic partitioning
  - YARN
  - Mesos
  - standalone scheduler

** Hadoop is actually 3 projects under Apache.
- HDFS
  - NameNode
  - DataNode
- YARN
- MapReduce
  - JobTracker
  - TaskTracker
    - Map
    - Reduce  

** Differences between spark and map-reduce
- Spark's purpose is not replace hadoop but to replace the map-reduce part of the Hadoop. The away you get parallelism in traditional map-reduce is you have multiple jvms and process ids for your map and reduce tasks. However in spark you get parallelism by having different threads running inside the executor.
- Map and reduce slots in traditional map-reduce are hard coded. The problem is initially when you start a map-reduce job the application is probably stuck for hours in map phase for hours and during this time the reduce slots are not used. Therefore we cannot see 90% CPU usage. In spark the slots are generic slots that can run map or reduce or any other thing.
- In traditional map reduce, once a map finishes what has to happen is the task tracker has to let the job tracker know that there is an empty slot so that it can assign a new map task. Therefore once a task finishes it could take 15-20 seconds before a new task lands in the slot (heartbeat b/w the job-tracker and task-tracker is 3 sec). So facebook built corona to overcome this issue. This is not a problem in spark. The reuse of slots in spark is significantly faster.
- spark actually calls the slots as cores.

** Local Mode
- start a scala/python shell
  - this starts a jvm
  - in this jvm you cache your rdd.
- setting the number of cores
  - assuming the machine has 8 cores, it a very common beginner mistake to say 2 for os and 6 for spark. This is a mistake
  - In general you should start with a factor of 2 or 3, which is equal to 12 or 18 task slots.
  - you can pass in a parameter at the start to of the shell to say how many threads you want in spark or spark starts with a default of one thread.
  - you pass * spark starts the threads which is equal to the number of logical cores, which is also by bad.
  - good setting is to use a factor of 2.
  - of course this might screw up the other services running on the machine. Therefore coming up with the right number of cores is an art of the system administrator.
- internal threads in spark.
  - roughly 20 internal threads running at anypoint in time. Mostly sitting ideal.
  - These are used for shuffle etc. When we need a shuffle they kick in, do the shuffle and go ideal again.
  - so if you try to align the number of logical cores with the number of tasks that won't work anyway because there are always internal threads running in spark.
  - shuffles in local mode are super fast as there are no data movement over the network.

** Standalone Mode
- In this mode when you start machines certain JVMs are going to instantiate. 
  - this is not starting an application
  - Master JVM
  - On each machine a worker JVM will start up
  - all the worker JVM will register with master and the master will know that there are 4 workers

The master and worker JVM are the resources managers. These JVMs are relatively small JVMs. They are relavtily ok with 500MB or 1GB of RAM.

Now we start the application:
- ssh into one of the machines.
- a driver starts in the machine where you ssh and asks the master for executors.
- The spark master works like a schedular to decide where it is going to launch the executor JVMs. Then tells each worker to launch the executor JVMs.
- All the master is doing is deciding and scheduling where the executors are going to run.
- All the worker jvms are doing is start the executors and if the executor crashes the worker restarts it. Thats why it doesnot need that much memory.
- If the worker jvm crashes the worker restarts it.
- You can make the driver highly available as well with a supervised tag, then if a driver fails the master restarts but if the driver restarts all the workers have to restart as well.
- The RDDs are cached in executors and the tasks are run in the executors. When the tasks are run local to an executor JVM which is what spark aims to do then it is quite fast, because the data from the executor JVMs heap right into the thread.
- You can also persist replicas on the RDDs.
- You can make the master highly available by adding more master jvms even while the application is running.
- If you are running multiple applications in standalone. You can have one worker start multiple executors, however one worker cannot start multiple executors for the same application.
  - This is a problem when you have machines with huge RAM. Because if we start a JVM with huge RAM then the garbage collection pauses gets really awkward.
  - Therefore if you want to utlize all the memory. Then you need to 2 or more workers and each worker on the machine can start a single executor.
  - SPARK_WORKER_INSTANCES (default = 1) this is how many worker jvms you want in each machine.
- SPARK_WORK_CORES (default = all) is the number of cores to allow spark applications to use on the machine. This is not the number of threads that will run in a worker JVM. It is actually the number of cores that the worker can give out to the underlaying executors. If you set this is 12 and one executor is started with 6 than it can start another executor with 6 cores. When you start an application in this mode and don't tell anything about the cores then the application will greedily acquire all the cores on the worker. By default it will whatever will take the max is.
- SPARK_WORKER_MEMORY (default = TOTAL RAM - 1GB) is how much the worker can give to its underlying executors. This is not setting the JVM heap the worker will use.
- SPARK_DEAMON_MEMORY (default = 512 MB) is the memory to allocate to spark master JVMs and the spark worker deamons themselves. The way you set the spark master and worker to set 1GB heaps is this one.
- Standlone mode settings
  - spark.cores.max: maximum of amount of CPU cores to request for the application from across the cluster.
  - spark.executor.memory: Memory for each executor

** YARN Mode

Yet Another Resource Negotiator

*** Architecture of YARN

**** Introduction

- There is a master machine running a resource manager. It contains
  - Schedular, which decides where the masters will run and the containers are scheduled
  - ApplicationS master, when an application master crashes this ApplicationS master will restart the application master and the application will have t o restart completely.
- Each slave machine in the hadoop cluster will run a node manager
- The node managers are heart beating with the Resource manager and sharing information live resource information.
  - How many cores are occupied
  - How much RAM is occupied
  - How much of the bandwidth is occupied
  This give the resource manager the ability to see how much resources are occupied across the cluster

**** Application Flow

You start the client application and submit it to the resource manager and it is gonna look at it and assume the responsibilty to negotiate a specific container to run on a node. 
- The application submitted will have some constraints for the application master and the resource manager will then find a node that has free resources that your application master needs and then start the application master through a container on that node.
- The application master is not going to do any work. It like the schedular in the traditional map reduce. What the application master will do is immediately contact the resource manager and say thanks for starting me but i need containers to do my actual work.
- The resource manager will then find the nodes that have the resources and then handover keys to the master with which the master can communicate with the node managers to start the containers.
- The node managers will accept the keys and will start the containers and register themselves with the application master and the application master will report this directly back to the client.
- After this point the containers are not in directly contact with the resource manager, so if the resource manager crashes the application will still be running. One problem if the resource manager crashes the application master will not be able to ask for new resources from the resource manger during the run time of this application. But if everything is fine the application master can be increasing and decreasing the number of containers while the application is running.
- With YARN the application master can make really detailed requests
  - One container with GPU
  - One container with CPU
  - Both containers should be on the same rack
  - Both containers should be on the same machine
  - Both container should have X amount of memory
 
*** Spark in YARN

There are 2 ways

**** client mode

- In client mode the driver runs on the client itself.
- The client can be your laptop.
- There are rest of thing like application master, containers, node managers, resource managers are still exactly as discussed above.
- However if you call an collect action on an RDD, the items inside the RDD will be shift to the driver over the network and you will be able to see the results in the shell. Not good if you collect the whole dataset.
- In this case the only reason the application master exsits is because in YARN that is the only way you can get containers to run executors. In this case the executors running in the containers are in direct communication in the driver.
- This is similar to the standalone mode where the worker start the containers, but here the node manager starts the containers and then starts the executors within it. But actually it is the application master that negotiates with the resource manager that it wants two containers to run executors.
- RM tells Application Master where the containers can be scheduled. The resource manager is basically doing what the spark master is doing in the standalone mode.
- However now there is one important schedular inside the spark driver which decides where the actual tasks will be scheduled. The driver schedular is going to aim for placing the tasks whereever the data is or closer to the data. This is different from the RM schdeular.
- Another interesting is to thing if you have n node cluster and you start less than n executors then problem is the hdfs blocks that only might exsit on the nodes in which the executors are not running might have to moved over network to another executor.
  - Better start n executors
  - if you start less than n executors, check for ways to start the executors where you can get more data locality.

**** cluster mode
- In client mode, if you remove the laptop from the cluster and go home, the driver is disconnecteed from the cluster which will terminate the cluster. So if you want to submit an application and walk away and see the results later (no interactive work) use cluster mode.
- In cluster mode the client submits the application including the python script or the jar file and the driver runs inside the spark application master itself, now we might not do something like collect but might save the results to the hdfs or somewhere.

*** Intersting settings in YARN
- --num-executors: controls how many executors will be allocated
  - cannot be done in stanalone mode
- --executor-memory: RAM for each executor
- --executor-cores: CPU cores for each executor

*** Dynamic Allocation:
Gives the ability to relatively increase or decrease the number of executors live in an application
- spark.dynamicAllocation.enabled
  - turns on the feature dynamic allocation
- spark.dynamicAllocation.minExecutors
  - starts this minimum number of executors
- spark.dynamicAllocation.maxExecutors
  - 
- spark.dynamicAllocation.sustainedSchedulerBacklogTimeout
  - after the starting the min number of executors and after this timeout spark checks if there are tasks that are not scheduled or waiting to be scheduled. If there are such tasks then few new executors are started if the current number of running executors is less then the max numbers of executors scheduled.
    - 10 -> 20
    - this timeout is only checked once
- spark.dynamicAllocaton.schedulerBacklogTimeout
  - spark checks for backlog again after this timeout and increases the executors exponentially.
  - 10 -> 20 -> 50 -> 100
  - this timeout is checked over and over again and increase the executors until the max is reached
- spark.dynamicAllocation.executorIdleTimeout
  - this timeout is to check if there are idle resources. if in the currently running executors if no task is scheduled spark starts to terminate the executors after this timeout hits until the min number of executors for the application is reached. 

This is how dynamic allocation works in YARN. There is something similar in mesos but a little different. This cannot be done in standalone mode

** Gist on resource schedular modes

|            | spark central master | who starts executors | tasks run in |
|------------+----------------------+----------------------+--------------|
| local      | none                 | human                | executor     |
| standalone | standalone master    | worker jvm           | executor     |
| YARN       | YARN App master      | Node Manager         | executor     |
| Mesos      | Mesos Master         | Mesos Slave          | executor     |

- the tasks are always are run in executors
- who starts the executors is different depending on the resource managers
- the spark central master is the guy decides where the executors will run. There are different deamons depending on the mode which decide where the executors will start. 

** Deploying an APP to the cluster

- Keep the code in the local and develop everything in local mode.
- Use spark submit to deploy the code in a cluster.

| Mode       | Value              | Explanation                                                                                                                                                 |
|------------+--------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| standalone | spark://host:port  | connect to a spark standalone master at the specified port (default port is 7707)                                                                           |
| mesos      | messos://host:port | connect to a mesos cluster master at the specified port. (default port is 5050)                                                                             |
| YARN       | yarn               | indicates submission to yarn clinet. whe running on YARN you'll need to export HADOOP_CONF_DIR to point the location of your Hadoop configuration directory |
| local      | local              | Run in local mode with single core                                                                                                                          |
| local      | local[N]           | Run in local mode with N cores                                                                                                                              |
| local      | local[*]           | Run in local mode with as many cores s the machine has                                                                                                      |

There are some other settings can be set like executor memory, number of executors etc. please refer to the docs

** Issues

*** Data locality

If you have a 100 node cluster start 100 executors so that you won't run into data locality issues. If you start less then data has to be over the network which can be a pain.
* General Rules of Thumb
** Configurations
- Recommended to use at most only 75% of a machine's memory for spark.
- Minimum executor heap size should be 8GB
  - giving it only a GB of memory you will run into old memory issues.
- Max executor heap size depends ... may be 40GB, this depends on how the JVMs GC is doing. Increasing this greater than 40GB will introduce GC issues. The GC will take longer than usual resulting in slowing down the jobs.
  - if usiing properitory jvms that can handle 100's of GB of memory this is fine may be.
  - In general if using open JDK or some other opensource jvms then likely better keep this under 40GB.
** Memory
- Memory usage is greatly affected by the storage level and serelization formats.
  - memory usage is greatly affected by how you store the rdds in memory.
  - cache is memeory only but persist you can pass in arguments.
    - RDD.cache() == RDD.persist(options)
      - MEMORY_ONLY (deserialized in memory, can be serailized)
      - DISK_ONLY (will always be serialized) the disk is SSD
      - MEMORY_AND_DISK (deserialized in memory and serialized in disk)
      - MEMORY_AND_DISK (default serialized in memory and disk)
      - MEMORY_ONLY_X (stores x replicas of the RDD in memory).
      - OFF_HEAP
    - If using serialization use a fast serialization (kyro). The default java serialization is slow.
    - The serialization might slow the app down a little bit because it involves the cost of serialization and deserialization. However if serialized the cost of GC reduced because many individual buffers are stored as a single java object.
    - Anything the moves to disk always serialized.
- when you persist the rdd in memory spark is storing the RDD is as deserailized java objects.
- when you cache the RDD is stored into the memory and the pieces that don't fit into the memory will be dropped will not be stored in the disk. However when the dropped RDD is required those pieces are recomputed from the underlying data source or an eariler RDD that you have cached in the DAG.
- If you cache an very big RDD then all the eariler RDD will removed from memory and 50 or so percent of this RDD might be cached. LRU (least recently used) cache.
- If a executor crashes the cached RDD in the JVM is lost but spark will not right way know what parts of the cached RDD are lost because it uses lazy evalution, but when required it goes back and recomputes the transformations from the underlying data source when required.
- When caching you can also say how many replicas you want to cache. In general you want a replication factor of 1 only. This is not like HDFS where you want to have multiple replicas of the same blocks of data. Just trust the recompute engine of spark to recreate the lost partition.
- A replication factor of more than 1 is needed only when the RDD is extermely important, for example when it takes a lot of compute to recreate a lost partition of the RDD because of a large dependencies on the previous RDDs.
** Persisting
- If RDD filts in memory. choose MEMORY_ONLY
- IF not, use MEMORY_ONLY_SER with fast serialization library.
- Don't spill to disk unless function that computed the datasets are very expensive or they filter a large amount of data. (recomputing may be as fast as reading from the disk).
- Use replicated storage levels sparingly and only if you want fast fault recovery (may be to serve requrests form a web app)
- Intermediate data is automatically persisted during shuffle operations. So next time if you call an action on the same RDD spark might walk back the DAG and start from this persisted data which was actually persisted during a previous shuffle operation.
* Other integrations into Spark
** Tachyon
- Theoritical particle faster than light.
- OFF_HEAP in-memory storage in for spark.
- The RDD will stored off heap in a searilized format which will reduce the garbage collection overhead in the JVM.
- Also if you store inside tachyon the RDD will still be available even if the executor JVM crashes and restarts.
- COOL THING: you write a scala ETL job and persist the data into tachyon and pick it up with a pyspark job and do something put it back into tachyon and pick it up with JAVA to do something else. This can be like the go between spark programming languages.
- It runs outside the executor process and hopefully in every node.
- Might be relativily slower because the RDD is not in the JVM, but relativily fast because it is not the RDD is still in memory and coming from an SSD or something even slower.
  - 
* Language Specific Details
** Pyspark
- stored objects will always be serialized with pickle library. so it does not matter whether you choose a serialized level. 
* Common issues.
** Memory Errors.
- Default memory allocation in spark
  - Cached RDDs (60% spark.storage.memoryFraction)
    - when you call .persist() or .cache(). Spark will limit the amount of memory used when caching to a certain fraction of the JVMs overall heap set by spark.storage.memoryFraction.
    - If you are not caching any RDDs you can reduce this to 10 or 20 percent. However it is good to always cache RDDs in memory.
  - Shuffle Memory (20% spark.shuffle.memoryFraction)
    - When performing shuffle operations. Spark will create intermediate buffers for storing shuffle output data. THese buffers are used to store intermediate results and aggreations in addition to buffering data that is going to be directly output as part of the shuffle.
  - User Programs (20% remainder)
    - Spark executes arbitrary user code. So user functions can themselves require substantial memory. For instance, if ause applicaition allocates large arrays or other objects, these will content for overall memory usage. User code has access to everything "left" in the JVM heap after the space for RDD storage and shuflle storage are allocated.
- Executor JVM crashing with memory errors
  - Error will be in one of logs where the linux kernel says it killed something because it is using up more memory than it is permitted.
  - Findout which one of the above 3 is causing the issue. (user programs or shuffle memory mostly).
    - user memory: there is no way you can set this in spark. One thing you can do is to reduce the spark storage memory fraction and spark shuffle memory fraction.
    - shuffle memory error: if an error happens during a shuffle then it is most likely a shuffle memory error. Try increasing the spark.shuffle.memoryFraction.
    - 
