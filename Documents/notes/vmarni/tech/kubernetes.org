#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup
#+STARTUP: showall
#+EXPORT_FILE_NAME: ../exports/kubernetes.html
#+OPTIONS: ^:nil

* Overview
** why we need kubernetes and what it can do?
- service discovery and load balancing
- storage orchestration
- automated rollouts and rollbacks
- automatic bin packing
- self-healing
- secrets and configuration management

*** what it is not?
- it is not a Paas solution. It works at a container level rather than hardware level.
- doesnot limit the types of applications. supports extremely diverse set of work loads. if an application can run of container it will run great on kubernetes
- doesnot provide application level services, such as middleware (messaging buses), dataprocessing framework (spark), databases, caches, cluster storage system as built in applications. Such applications can run on kubernetes and/or can be accessed by applications running on kubernetes through protable mechanisms such as the open service broker.
- doesnot dictate logging, monitoring or alerting solutions. it provides some applications as proof of concept and mechanisms to collect and export metrics
- etc etc
- additionally its not a mere orchestration system. it infact elimnates the need for orchestration.

/orchestration/ is technically execution of an workflow. A-->B-->C. In constract kubernetes is a set of independent composable processes that drive the current state towards the desired state. should not matter how you get from A-->C.

** Components

*** Master components

- Provides the clusters control panel. Makes global decisions about the cluster and they detect and respond to the cluster to events.
- master components can be run independently on any machine in the cluster. however for simplicity set up scripts typically start all master components on the same machine and donot run user containers on the same machine.

**** Kube-api server

- component on the master that exposes that kubernetes api. it front-end of the kubernetes control plane.
- it is designed to scale horizontally that is it scales by deploying more instances.
**** etcd
- consistent and highly available key-value store used as kubernetes backing for all cluster data.
**** kube-scheduler
- component on the master that watches newly created pods that have no node assigned, and selects a node for them to run on.
**** kube-control-manager
- component on the master that runs controllers.
- logically each controller is a separate process, but to reduce complexity they are all compiled into a single binary and run in a single process.
- These controllers include.
  - Node controller: Responsible for noticing and responding when nodes go down.
  - Replication controller: Responsible for maintaining the correct number of pods for every replication controller object in the system.
  - Endpoint controller: populates the endpoint objects. (that is joins service and pods)
  - Service account & token controllers: create default accounts and API access tokens for new namespaces.
**** TODO cloud-control-manager
*** Node components
Node components run on node, maintaining running pods and providing the kubernetes runtime env.
**** kubelet
An agent that runs on each node in a cluster. it make sures that containers are running in a pod. A kublet takes a set of PodSpecs that are provided through various mechanisms and ensures that containers described in those PodSpecs are running and healthy. The kubelet doesn't manage containers which are not created by kebernetes.
**** kube-proxy
kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the kubernetes service concept. Kube proxy maintains the network rules on nodes. These rules allow network communication to your pods from network sessions inside or outside your cluster.
**** container-runtime
Container runtime is a software that is responsible for running containers. Kubernetes supports several container runtimes
- Docker
- containerd
- cri-o
- rklet
- any implementation of the kubernetes cri
*** Add ons
Addons use kubernetes resources (DaemonSet, Deployment, etc) to implement the cluster features. Because these addons are cluster-level features namespaced resources for addons belong within the kube-system namespace. Some of them are below.
**** DNS
while all other addons are not strictly required, all kuberenetes clusters should have a cluster DNS. Cluster DNS is DNS server in addition to other DNS servers in your environment which servers dns records for kubernetes services. Containers started by kuberenets automatically include Cluster DNS in their DNS searches.
**** Web UI
Dashboard is a general purpose, web based UI for kubernetes clusters. It allows users to manage and troubleshoot applications running in the cluster as well as the cluster itself.
**** Container Resource Monitoring
CRM records generic time-series metrics about the containers and provids a UI for browsing data.
**** Cluster Level Logging
A CLL is responsible for saving container logs to a central log stor with search/browsing feature.
*** Add-ons
* Architecture
** Nodes

A node is a worker machine in kubernetes previously know as a minion. A node may be a VM or physical machine, depending on the cluster. Each nodes contains the services to run pods and is managed by the master components. The services on a node include the container runtime, kubelet and kubelet proxy. 
*** Node Status
contains the following information.
- Addresses
  - Hostname
  - external ip
  - internal ip
- conditions
  - OutOfDisk
  - Ready
  - MemoryPressure
  - PIDPressure
  - DiskPressure
  - NetworkUnAvailable
- capacity and allocatable
  - CPU
  - memory
  - number of pods that can be scheduled onto the node.
- info
  describe general info such as
  - kubernetes version (kubelet and kube-proxy version)
  - docker version (if used)
*** Management
unlike pods & services node is not externally created by kubernetes, it created externally by cloud providers or it exists in a pool of virtual or physical machines. So when kubernetes creates a node:
- it creates an object that represents the node.
- after creation, kubernetes checks wheather the node is vaild or not.
  - if all the services are running then the node is valid and can be used to run pods.
- otherwise it is ignored for any cluster activity until it becomes active.

Currently there are 3 interfaces that interact with the kubernetes node interface: node controller, kubelet, kubectl.

**** Node Controller
It is the master component which manages various aspects of a nodes.
The node controller has various aspects in node life.
- first is assigning a CIDR block when the node id registered.
- keeping node controllers internal list of nodes up to date with the cloud providers list of available machines.
- when running in a cloud environment, whenever a node is unhealthy the node controller asks the cloud provider if the VM for the node is still available. If not the node controller deletes the node from the list of nodes.
- Monitoring the node condition. The node controller is responsible for updating the NodeReady condition of NodeStatus to ConditionUnknown when the node becomes unreachable and later evicting all the pods from the node if the node continues to be unreachable. 

**** TODO Self Registration of Nodes

**** Node Capacity
The capacity of the node is part of node object. Normally node register themselves and report their capacity when creating the node object. Kubernetes scheduler ensures that there are enough resources for all pods on a node. It checks that the sum of the requests of containers on the node is no greater than the node capacity. It includes all the containers started by the kubelet, but doesn't include any containers started by the container runtime nor any process running outside the containers.

** Master-Node Communication

communications paths between the master(api-server) and the kubernetes cluster. 

*** cluster to master

- All communication paths  from the cluster to the master terminate at the apiserver. None of the other master components are desiggned to expose remote services.
- Nodes should be provisioned with public root ceritificate for the cluster such that they can connect securely to the apiserver along with valid credentials.
- Pods that wish to connect to the apiserver can do so securely ny leveraging a service account so that kubernetes will automatically inject the public root certificate and a valid bearer token into the pod when it is instantiated. The kubernetes service (in all namespaces) is configured with a virtual IP address that is redirected (via kube-proxy) to the https endpoint on the apiserver.
- The master components also communicate with the cluster apiserver over the secure port.
- As a result the default operating mode for connections from the cluster (nodes and pods running on the nodes) to the amster is secured by default and can run over untrusted and/or public networks.


*** Master to Cluster

- 2 primary communication paths from the master(apiserver) to the cluster. 
  - from the apiserver to the kubelet process which runs on each node in the cluster.
  - from the apisrver to any node, pod or service through the apiserver's proxy functionality.

**** apiserver to kubelet
 
 these connections are used for:
 - fetching logs for pods.
 - attaching (through kubectl) to running pods.
 - providing the kubeletes port-forwarding functionality

these connectionss terminate at the kubelets https endpoint. By default, the apiserver doesnot verify the kubelets serving cerfificate, which makes the connection subject to man-in-the-middle attacks and unsafe to run over untrusted and/or public netowrks.

***** ssh tunnels (deprecated)

kubernetes supports ssh tunnels to protect the master cluster communication paths. In this configuration, the apisrver initiates an ssh tunnel to each node in the cluster and passes all traffic destined for a kubelet, node, pod or service through the tunnel. This tunnel ensures that the traffic is not exposed outside of the network in which nodes are running.

**** apiserver to nodes, pods and services

the connections from the apiserver to a node, pod or service default to plain HTTP connections and are therefore neither authenticated nor encrypted. They can be run over a secure HTTPS connection by prefixing https: to the node, pod or service name in the API URL, but they will not validate the certificate provided by the HTTPS endpoint nor provide client credentials so while the connection willbe encrypted, it wil not provide any gurantees of integrity. These connections are not currently safe to run over untrusted and/or public networks.

** Concepts underlying the cloud controller manager

The cloud controller manager (CCM) runs alongside with other master components such as the kubernetes controller manager, the API server and scheduler. It can also be started as a kubernetes addon, in which case it runs on top of kubernetes. The cloud controller managers design is based on a plugin mechanism that allows new cloud providrs to integrate with kubernets easily by using plugins. There are plans in place for on-boarding new cloud providres on kuberentes and for migrating cloud providres from the old model to the new CCM model.

- Architecture of kubernetes cluster without the CCM.
  #+CAPTION: kubernetes cluster without CCM
  [[./images/kubernetes-cluster-without-ccm]]
- Architecutre of kubernetes cluster with CCM.
  #+CAPTION: kubernetes cluster with CCM
  [[./images/kubernets-cluster-with-ccm]]

  In the architecture without CCM, Kuberenetes and the cloud provider are integrated through several different components.
  - Kubelet

  - kubernetes controller manager

  - kuberentes API server

The CCM consolidates all of the cloud-dependent logic from the preceding 3 componenets to create single point of integration with the cloud. 

*** Components of the CCM

The CCM breaks away some of the funcationality of kubernets controller manager (KCM) and runs it as a separate process. Specifially, it breaks away those controllers in the KCM that are cloud dependent. The KCM has the following cloid dependent controller loops:
- Node Controller
- Volume Controller
- Route Controller
- Service Controller

*** Functions of the CCM

The CCM inherits its functions for components of kubernetes that are dependent on a cloud provider.

**** Kubernetes Controller Manager

The majority of the CCMs functions are derived from the KCM. As mentioned in the previous section, the CCM runs the following control loops
- Node controller
- Route controller
- Service controller

***** Node Controller
initializing a node by obtaining information about the nodes running in the custer from the cloud provider.

Functions
- initialize a node with cloud specific zone/region labels.
- initialize a node with cloud specific instance details, for example, type and size.
- obtain the nodes network addresses and hostname
- In case a node becomes unresponsive, check the cloud to see if the node has been deleted from the cloud. If the node has been deleted from the cloud, delete the kubernetes Node object.

***** Route Controller
The route controller is responsible configuring rotes in the cloud appropriately so that containers on different nodes in the kuberenetes cluster can communicate with each other. Only applicable for Google Compute Engines Clusters.

***** Service Controller
The service controller is reponsible for listening to service create, update and delete events. Based on the current state of the kubernetes, it configures cloud load balancers (ELB, Google LB, Oracle Cloud Infrastructure LB) to reflect the state of the services in Kubernetes.

**** Kubelet

The Node controller contains the cloud-dependent functionality of the kubelet. Prior to the introduction of the CCM, the kubelet was responsible for intializing a node with cloud-specific details such as IP, region/zone labels and instance type information. The introduction of the CCM has moved this initialization operation from the kubelet into the CCM. In the new model the kubelet initializes the node without the cloud specific configuration. However, it adds a taint to the newly created node that makes the node unschedulable until the CCM initializes the node with cloud specific information it then removes this taint.

*** Plugin Mechanism
The CCM uses Go interfaces to allow implementations from any cloud to be plged in. The implementation of the four shared controllers highlighted above and some scaffolding along with the shared cloudprovider interface, will staty in the kubernetes code. Implementations specific to cloud providres willbe built outside kubernetes coere and implement interfaces defined in the core.

*** Authorization
breaks down the access required on various API objects by the CCM to perform its operations.

**** Node controller
The Node controller only works with Node objects. It requires full access to
- get
- list
- create
- update
- patch
- watch
- delete

**** Route controller
The route controller listens to Node object creation and configures routes appropriately. It requires 
- Get

**** Service controller
The service controller listens to service object
- create
- update
- delete
then configures the end points for those services appropriately. To access services it requires patch and update access. To setup endpoints for the services, it requires access to
- list
- get
- watch
- patch
- update

* Containers 

** Images

 You create your docker images and push it to a registry before referring to it in a kuberenetes pod. The image property of a container supports the same syntax as the docker command does, including private registeries and tags
 - updating images
 - building multi-architecture images with manifests
 - using a private registry

*** TODO updating images

*** TODO building multiarchitecture images with manifest

*** using private registry

Private registeries may require keys to read from them. Credentails can be provided in several ways:

**** using goodle container registry
- per cluster
- automatically configured on google compute engine or google kubernetes engine
- all pods can read the projects private repo

**** using aws ecr
- use IAM roles and policies to control access to ECR repositories
- automatically refresh ECR login credentials

**** Configuring the nodes to authenticate to a private registry
- all pods can read any configured private registries
- requires node configuration by cluster administrator

**** prepulled images
- all pods can use any images cached on the node
- requires root access to all nodes to setup

**** specifying imagePullSecrets on Pod
- only pods which provide own keys can access the private registry

*** Using amazon elastic container registry
 kubernetes has native support for the aws ecr, when nodes are aws ec2 instances. Simply use the full image name in the Pod definition. All users of the cluster who can create pods will be able to run pods that use any of the images in the ECR registry. The kubelet will fetch and periodically refresh ECR credentials. (need some permissions to do this)

*** Use Cases
There are a number of solutions for configuring private registries. Here are some common use cases and suggested solutions.

/note: if you need to access multiple registries you can create one secret for each registry. Kubelet will merge any imagePullSecrets into a single virtual .docker/config.json/

**** Cluster running only non-proprietary images. No need to hide images

- use public images on the docker hub
  - no configuration need
  - on GCE a local mirror is automatically used to speed up the look up.

**** Cluster running some properitary images, which should be hidden to those outside the company, but visible to all the users in the company.
- use a hosted private docker repo
  - it may be hosted on the docker-hub or elsewhere
  - manually configure docker/config.json on each node.
- or run an internal private registry behind your firewall with open read access
  - no kubernetes configuration is required
- etc etc

**** cluster with proprietary images, a few of which require stricter access control
- ensure AlwaysPullImages admission controller is active, Otherwise all Pods of all tenants potentially have access to all images.
- Run a private registry with authorization required.
- Generate registry credential for each tenant, put into secret and populate secret to each tenant namespace.
- The tenant adds the secret to imagePullSecerts of each namespace.


** Container Runtime Variables

*** Container Env
The kubernetes container env provides several important resources to containers.
- A file system, which is a combination of an image and one or more volumes.
- Information about the container itself
- Information about other objects in the cluster

*** Container Information
- The hostname of a container is the name of the Pod in which the container is running. It is available through the hostname command or the gethostname function call in libc.
- The Pod name and namespace are available as environment variables through the downward API.
- User defined environment variables from the Pod definition are also available to the container as are any environment variables specified statically in the docker image.

*** Cluster Informatin
A list of all services that were running when a container was created is availble to that container as environment variables. Those env variables match the syntax of Docker links. Services have dedicated IP address and are available to the container via DNS, if DNS addon is enabled.

** Runtime Class
RuntimeClass is a feature for selecting the container runtime configuration. The container runtime configuration is used to run a Pods containers.
*** Motivation
- You can set a different RuntimeClass between different Pods to provide a balance of performance versus security. For example, if part of your workload deservers a highlevel of information security assurance, you might choose to schedule those pods so that they run in a container runtime that uses hardware virtualization. You did then benefit from the extra isolation of the alternative runtime, at the expense of some additional overhead.
- You can also use RuntimeClass to run different Pods with the same container runtime but with different settings.
** Container Lifecycle Hooks
how kubelet managed containers can use the container lifecycle hook framework to run code triggered by events during their management lifecycle. The hooks enable containers to be aware do events in their management lifecycle and run code implemented in a handler when the corresponding lifecycle hook is executed.
*** Container Hooks
Ther are 2 hoks that are exposed to containers
- PostStart
- PreStop
*** Hook Handlers
Containers can access a hook by implementing and registering a handler for that hook. There are 2 types of hook handlers that can be implemented for containers.
- Exec: executes a specific command, such as pre-stop.sh, inside the cgroups and namespaces of the contianer.
- HTTP: Executes an HTTP request against a specific endpoint on the container.
